---
output:
  word_document: default
  html_document: default
  '': default
---

---
title: "CIS 585 Project"
author: "Kodai Watanabe"
date: "05/05/2022"
output: word_document
---


## Abstract 

In this project, I try to find an answer for a research question, How does years of experience of developers affect their compensation. Figuring out the answer is beneficial for HR people and job seekers. It is expected that HR people can use the finding to provide a better estimate of compensation and the job seekers can know how much they should expect as salaries based on their experience. 

It is argued that more experience lead to higher compensation in general. In this research, I examine the relationship in the context of developers. To implement the research, I get data from a survey conducted by Stack Overflow in 2021 and did a regression analysis. I find that for each one percent increase in the professional experience as a developer, there is an increase of 0.181% in the compensation amount. In the regression analysis, the length of mere coding experience does not significantly affect the compensation from a standpoint of statistics. 

However, this relationship between the professional experience and the compensation is applied to only developers who make less than a compensation amount of 12 in the logarithmic scale. This limitation arises because of non-linearity in the data. 

Based on the findings, I would recommend HR people to focus on the professional experience of coding rather than mere coding experience when estimating the compensation. More professional experience causes the increase of the compensation, so failing to pay enough money for certain professional experience lead to not be able to hire a good candidate. 

For job seekers, I would encourage to gain internships to gain professional experience. 

## Introduction 

This research project tries to answer a question, how does years of experience of developers affect their salaries in the U.S.? Years of experiences are generally considered one of factors that determine the amount of salary. This is because, the longer you do a job, the more productive you become (Torpey, 2015). It is mentioned by an article that it is typical to get paid more if you have more experience (Salary.com, n.d.). In job postings of developers, some years of experience, such as 5 years, is often seen as a requirement. This research project will test how significant the factor is for the determination of salary of developers in the U.S.

This research project is important for two reasons. One is that the findings can give people in HR an insight about how impactful years of experience are on a figure of salary. They could provide more accurate estimate of salary based on the findings. Second is that the results of the project could help job seekers and students to determine whether their years of experience are valid against salaries shown by employers. 


## Literature Review 

You get paid more as you gain more experience even though this is observed in years of experiences up to 25 years; the base salary in the U.S. increases by $1500 for one-year increase in years of experience on average (Sauro, 2014). In a study focusing on the Indian IT industry, it is suggested that salary is positively related with work experience (Dash et al., 2017). 

Other factors also affect the amount of salary. For instance, education is one of them. Workers who possess higher degree, professional certification, or license might get paid more than others even though it is about the same industry (Torpey, 2015).

## Theory 

The years of experiences as developers increases the amount of their compensation, but the slope of the growth get flattened. 

## Data 
I get data from a survey conducted by Stack Overflow in 2021. 

https://insights.stackoverflow.com/survey

## Methodology 

From this section, I show all of what I did in this project to arrrive the conclusion I discussed earlier. 


Install Packages 
``` {r pressure, eval=FALSE}
install.packages("rmarkdown")
install.packages("knitr")
install.packages("ggplot2")
install.packages("plyr")
install.packages("dplyr")
install.packages("stringr")
install.packages("caret")
install.packages("car")
install.packages("lmtest")
install.packages("sandwich")
install.packages("magrittr")
install.packages("stargazer")
install.packages("leaps")
```



Load packages 
```{r warning=FALSE, results='hide'}
library("rmarkdown")
library("knitr")
library("ggplot2")
library("plyr")
library("dplyr")
library("stringr")
library("car")
library("lmtest")
library("sandwich")
library("magrittr")
library("stargazer")
library("caret")
library("leaps")
```


## Read data 
I download a dataset from Stack Overflow 2021 survey. 

``` {r read_data}
dataset <- read.csv('Datasets/survey_results_public.csv')
```

See what the data looks like. 
First, I check data types of columns. Some data types need to be transformed. For instance, Response ID should have character data type. 
``` {r check_datatypes }
str(dataset)
```


Count null values for each of the columns 
```{r count null}
null_counts <- sapply(dataset, (function(x) sum(is.na(x))))
null_counts
```


Get respondents who are developers by profession and live in the U.S.because they are what I am interested in. 
```{r get respondents}
dataset <-subset(dataset, MainBranch=='I am a developer by profession' & Country=='United States of America')
```

## Data Cleaning 

From here, I start cleaning the data. 

First, I drop definitely unnecessary columns. I drop columns related to this survey. 
They should not be correlated with the compensations of the respondents. 

```{r drop unncessary columns}

dataset <- subset(dataset, select=-c(SurveyLength,SurveyEase,UK_Country))

```


Change the data type of ID column. To prevent from applying a calculation on the column, I convert it into character data type. 
```{r change dtype of ID}
dataset$ResponseId <- as.character(dataset$ResponseId)
```



Looking at YearsCode column, there are two string entries, less than 1 year and more than 50 years. 
They occupy only 1.2% of all respondents. Thus, they can be considered outliers. So, I drop rows containing the entries. 
Then, I change the column's data type into numeric. 
```{r YearsCode}
unique(dataset$YearsCode)
dataset <- dataset[!dataset$YearsCode %in% c("More than 50 years","Less than 1 year"),]
dataset$YearsCode <- as.numeric(dataset$YearsCode)
```
Same logic is applied to YearsCodePro column. 
```{r YearsCodePro}
unique(dataset$YearsCodePro)
dataset <- dataset[!dataset$YearsCodePro == "Less than 1 year",]
dataset$YearsCodePro <- as.numeric(dataset$YearsCodePro)
```

Let's look at OrgSize column. 
One of the unique values is a very long string. Let's rename it. I call it Other. 

```{r}
unique(dataset$OrgSize)
dataset$OrgSize <- mapvalues(dataset$OrgSize,from=c("Just me - I am a freelancer, sole proprietor, etc."),
                             to=c("Other"))
```


About EdLevel, most of values of this variable are very long descriptions. Let's rename them. 

```{r edlevel}
unique(dataset$EdLevel)
from <- c("Bachelorâ€™s degree (B.A., B.S., B.Eng., etc.)","Other doctoral degree (Ph.D., Ed.D., etc.)" ,
          "Masterâ€™s degree (M.A., M.S., M.Eng., MBA, etc.)", "Associate degree (A.A., A.S., etc.)",
          "Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)",
          "Some college/university study without earning a degree" ,
          "Primary/elementary school",
          "Professional degree (JD, MD, etc.)",
          "Something else" )
to <- c("Bachelor","Doctorate","Master","Associate","Secondary School","College study wihtout degree","Elementary school",
        "Professional degree","Something else")
dataset$EdLevel <- mapvalues(dataset$EdLevel,from,to)
```

Regarding Employement, this variable also contains a very long string. Let's rename it. 
```{r}
unique(dataset$Employment)

dataset$Employment <- mapvalues(dataset$Employment,from=c("Independent contractor, freelancer, or self-employed"),to=c("Other"))
```

There are columns regarding tools developers want to work with and have worked with. I drop columns that show tools they want to work with. 
It is hard to imagine that tools they want to use are related to their current compensation. On the other hand, columns about tools they have worked with might be useful. They could describe how experienced the developers are. I deal with this type of columns in the later section.  

```{r Want to work columns}
wanto_columns <- grepl(pattern = "WantToWorkWith", x=names(dataset))
dataset <- dataset[,!wanto_columns]

```


There are columns about Stack Overflow.The usage of the site has nothing to do with the compensation amount, so let's drop the columns. 
```{r drop stack overflow columns}
dataset<- subset(dataset,select=-c(NEWSOSites,SOVisitFreq,SOAccount,SOPartFreq,SOComm))
dataset<- subset(dataset,select=-c(NEWOtherComms))
dataset<- subset(dataset, select=-c(NEWStuck))
```


Drop a column which tell what kinds of OS they use. It is irrelevant to the research question. 
```{r drop os}
dataset <- subset(dataset, select=-c(OpSys))
```

Let's go back to the worked with columns. Those columns contain tools' names they have used. 
I convert the list of names into the number of tool. s
First, let's define a function to calculate the number of tools. 
```{r calculate tools func}
calculate_elements <- function(elements)
{
    if(any(is.na(elements))){
      return (0)
    }
    else{
      return (length(elements))
    }
    
}

```




Then, apply the function to the worked with columns. 
```{r create columns}
columns <- names(dataset)[grep(pattern = "HaveWorkedWith",x=names(dataset))]


for(col_name in columns)
{
  dataset[,col_name] <- rapply(strsplit(dataset[,col_name],";"),calculate_elements)

}
```


Drop rows that contain NA values 

```{r drop nas}
dataset <- na.omit(dataset)
```


I do not believe that how people learn code is crucial in deciding the compensation amount. So, let's drop a column about how they learned coding. 
```{r drop how people learn codes}
dataset <- subset(dataset, select=-c(LearnCode))

```


Types of developers could affect the amount of compensation. For instance, Data Scientists could earn more than front-end developers. 
In this survey, one respondent can choose more than one type of developers. Thus, for each developer type available in this survey, let's create a binary column which tells a person belongs to the type. 

First, get all dev types available in this survey 
``` {r devtypes}

all_types =c()
for (types in dataset$DevType){
  for(type in strsplit(types,";")){
    for(content in type){
      if(!(as.character(content) %in% all_types)){
        all_types <-append(all_types,as.character(content))
    }
    }
  }
}
```

Then, create the columns in the original data set. 
```{r}

dev_types <- setNames(data.frame(matrix(0,ncol = length(all_types), nrow = nrow(dataset))), all_types)
dataset<- cbind(dataset,dev_types)

for(id in dataset$ResponseId){
  job_desc <- dataset[dataset$ResponseId==id,"DevType"]
  description <- strsplit(job_desc,";")
  for(outer_list in description)
  {
    for(inner_list in outer_list)
    {
      if (inner_list == "Other (please specify):")
      {
        dataset[dataset$ResponseId==id,"Other (please specify):"] <- 1
      } else{
        dataset[dataset$ResponseId==id,inner_list] <- 1
      }
    }
  }
}
```


This is what the columns look like. 
```{r}
head(dataset[c('Developer, full-stack','Engineer, data','Data scientist or machine learning specialist')])
```

Clean names of columns 
```{r clean names of columns}
colnames(dataset) <- gsub(" ","_",colnames(dataset))
colnames(dataset) <- gsub(",","",colnames(dataset))
colnames(dataset) <- gsub("-","_",colnames(dataset))
colnames(dataset) <- gsub("/","",colnames(dataset))
```


Rename some of column names.
```{r}
dataset$Other_devtypes <- dataset$`Other_(please_specify):`
dataset$Senior_excecutives <- dataset$`Senior_Executive_(C_Suite_VP_etc.)`

dataset <- subset(dataset, select=-c(`Other_(please_specify):`,`Senior_Executive_(C_Suite_VP_etc.)`))
```

Drop the original dev types column 
```{r drop dev type}
dataset <- subset(dataset, select=-c(DevType))
```

There is a sexuality column. I do not think that a sexual preference significantly affects the amount of salary. Thus, I will drop the column.
```{r drop sexuality}
dataset <- subset(dataset,select =-c(Sexuality))
```


## Looking at each variable 

### Gender 
Some respondents choose more than one gender categories. I think that it will be too detailed if I include all of these information. Since majority of respondents is male and the second biggest proportion is female. I will keep only male and female respondents. 


```{r }
dataset %>%group_by(Gender) %>%summarise(cnt = n())%>%mutate(percentage=(cnt/sum(cnt))*100)%>%head()

```
```{r}
dataset<- dataset[(dataset$Gender=="Man") | (dataset$Gender=="Woman"),]
```


### Ethnicity 
This Ethnicity categorical column is also very imbalanced. Most of values are white. Proportions of other unique values spread out. 
The problem is that the distinction is too detailed. Moreover, some of respondents have more than one Ethnicity even though there is a option called multiracial. 
Since I don't know how to deal with multiple entries of ethnicity for one respondent and most of respondents have one ethnicity type, 
I will pick only rows that contain one value for this variable. 
```{r}
unique(dataset$Ethnicity)[1:10]
dataset %>%group_by(Ethnicity) %>%summarise(cnt = n())%>%mutate(percentage=(cnt/sum(cnt))*100)%>%arrange(desc(percentage))%>%head(20)
dataset <- dataset[(rapply(strsplit(dataset$Ethnicity,";"),length)==1),]
```



Now the variable looks like this 
```{r}
unique(dataset$Ethnicity)
```

Since the entry of indigenous is too long, let's make it shorter. 
```{r}
dataset$Ethnicity <- mapvalues(dataset$Ethnicity, from=c("Indigenous (such as Native American, Pacific Islander, or Indigenous Australian)"), to=c("Indigenous"))
```


### Accessibility 
This variable is too detailed as well. Moreover, since respondents in this dataset have jobs, their disabilities are not so severe that they cannot work as developers if they have. 
However, the disabilities might affect efficiency of their work. If I make dummy variable for each of disabilities, it will be too detailed. So, let's make this column into a binary column that indicates whether people have a disability or not. 


```{r access, result='hide'}
unique(dataset$Accessibility)[1:10]
dataset %>%group_by(Accessibility) %>%summarise(cnt = n())%>%mutate(percentage=(cnt/sum(cnt))*100)
```

Convert the column into a binary column
```{r}
dataset[dataset$Accessibility=="None of the above","Accessibility"] <- 0
dataset[dataset$Accessibility!=0,"Accessibility"] <- 1
```


### Mental Health 

Apply same logic as what I used for the accessibility variable 
```{r Metal Health, results='hide'}
unique(dataset$MentalHealth)
```

```{r}
dataset %>%group_by(MentalHealth) %>%summarise(cnt = n())%>%mutate(percentage=(cnt/sum(cnt))*100)%>%arrange(desc(percentage))%>%head(5)

```

Create a binary column 
```{r}
dataset[dataset$MentalHealth=="None of the above","MentalHealth"] <- 0
dataset[dataset$MentalHealth!=0,"MentalHealth"] <- 1
```

### Currency 

Let's look at currency. A very few respondents get paid in non-us dollars. Since I selected only developers in the United States, this does not sound intuitive. Given that the number is very few and the situation is strange, I guess they are wrong entries. Let's drop the rows that contain non-us currency 

```{r}

unique(dataset$Currency)

```

```{r}
dataset %>%group_by(Currency) %>%summarise(cnt = n())%>%mutate(percentage=(cnt/sum(cnt))*100)%>%arrange(desc(percentage))
dataset<- dataset[dataset$Currency == "USD\tUnited States dollar",]
```

### Trans

Let's get rows containing Yes or No
```{r Trans }
unique(dataset$Trans)
```
```{r }
dataset <- dataset[(dataset$Trans=="No"|dataset$Trans=="Yes"),]
```


I use ConvertedCompYearly as a dependent variable. Let's again drop some variables that are not necessary for our project. 
```{r}
dataset <- subset(dataset,select=-c(Currency, CompTotal, CompFreq))
dataset <- subset(dataset,select=-c(MainBranch,Country))
dataset <- subset(dataset,select=-c(ResponseId))
```


Okay. Let's end our preliminary data cleaning here. We will move onto some data analysis. 
First, let's look at the dependent variable. 

## Data Analysis

It seems that the variable is skewed a lot. Median and mean are totally different. We need to consider dropping some observations. 

```{r summary_dependent}
summary(dataset$ConvertedCompYearly)
```


Let's look at the box plot. Since the distribution is very skewed, it is impossible to see the whole distribution. 
```{r boxplot_dependent}
ggplot(dataset,aes(x=0,y=ConvertedCompYearly))+
  geom_boxplot()
```



Let's see a relationship between the dependent variable and a variable of interest. 
Due to very high compensation amount, we can't see anything about the most of values of the dependent variable. Let's divide the variable into two groups, high and low. 
```{r YearsCode_Dependent}
ggplot(dataset)+
  geom_point(aes(x=YearsCodePro,y=ConvertedCompYearly))
```
```{r divide}
high_comp <-dataset[dataset$ConvertedCompYearly>=2.5e+06,]
low_comp <-dataset[dataset$ConvertedCompYearly<2.5e+06,]
```

Let's visualize them. 
This is high compenstaion. There might be a linear pattern in this. 
```{r high_comp}
#let's see plots and correlatino coefficient 
ggplot(high_comp)+
  geom_point(aes(x=YearsCodePro,y=ConvertedCompYearly))
```

This is low compensation. It looks like that there are still some outliers. Majority of values are under 500000. Since I would like to find a pattern that many observations follow, in this research, I focus on the majority values. 

```{r lowcomp}
ggplot(low_comp)+
  geom_point(aes(x=YearsCodePro,y=ConvertedCompYearly))
```

Get majority of the dependent variable and plot them. When looking at this, there is a non linear pattern. The compensation amount goes up by a certain point and move horizontally(or maybe decrease)
```{r }
majority_part <-dataset[dataset$ConvertedCompYearly<500000,]
ggplot(majority_part,aes(x=YearsCodePro,y=ConvertedCompYearly))+
  geom_point()+
  geom_smooth(method = "lm")+
  geom_smooth()
```

Let's apply log transformation on the independent variable. This is more linear. 
```{r}
ggplot(majority_part,aes(x=log(YearsCodePro),y=ConvertedCompYearly))+
  geom_point()+
  geom_smooth(method="lm")
```
Apply it on both of dependent and independent. This is great. 
```{r}
ggplot(majority_part,aes(x=log(YearsCodePro),y=log(ConvertedCompYearly)))+
  geom_point()+
  geom_smooth(method="lm")
```


Given these plots, I think that it would be better to focus on the majority of data point because the original dependent variable is very skewed and there is likely a patter in the majority of data points. 

Look at the distribution of the majority part. There are some data points that can be considered outliers(black points). I think that it is okay to actually consider them outliers. 

```{r boxplot_majority}
ggplot(dataset,aes(x=factor(0),y=ConvertedCompYearly))+
  geom_boxplot()+
  ylim(0,500000)
```


Points outside of the whiskers are considered as outliers in the boxplot. So, Let's calculate the upper and lower bound of the whiskers. 

```{r whiskers}
upper_limit <-(173000-95000)*1.5 + 173000
lower_limit <-(173000-95000)*1.5- 95000
```

Okay. Then, let's get rows that are not outliers. 
```{r fileter_rows}
dataset <- dataset[dataset$ConvertedCompYearly<=upper_limit & dataset$ConvertedCompYearly>=lower_limit ,]

```

Then, let's see the scatter plots again 
```{r}

ggplot(dataset,aes(x=YearsCodePro,y=ConvertedCompYearly))+
  geom_point()+
  geom_smooth(method="lm")+
  geom_smooth()
```

Log transformation on the dependent variable 
```{r}
ggplot(dataset,aes(x=log(YearsCodePro),y=ConvertedCompYearly))+
  geom_point()+
  geom_smooth(method="lm")+
  geom_smooth()
```

Log transformation on both of the variables. Even after log-log transformation is applied, from a certain point, the pattern becomes non linear. The compensation becomes constant or decreased from a certain point of years of professional coding experience. 
```{r}
ggplot(dataset,aes(x=log(YearsCodePro),y=log(ConvertedCompYearly)))+
  geom_point()+
  geom_smooth(method = "lm")+
  geom_smooth()
```

Alright, let's see a relationship between the dependent and the each of the independent variables. 

By Employement 
```{r employement}
ggplot(data = dataset, mapping = aes(x =Employment, y =ConvertedCompYearly))+
  geom_boxplot(horizontal=TRUE)+
  coord_flip()
```

By States 
```{r state, fig.width=12, fig.height=12}
group_by_state <- group_by(dataset,US_State) %>% summarise(median=median(ConvertedCompYearly))
ggplot(data=group_by_state, mapping=aes(x=reorder(US_State,median),y=median))+
         geom_bar(stat="identity")+
  xlab("US states")+
  ylab("Median Compensation")+
  coord_flip()
```

By Education Level 
```{r }
ggplot(data = dataset, mapping = aes(x = reorder(EdLevel,ConvertedCompYearly, FUN=median), y =ConvertedCompYearly))+
  geom_boxplot()+
  xlab("Education Level")+
  coord_flip()

```
Age1stCode 
This variables is about time when respondents write the first code. 
```{r}
ggplot(data = dataset, mapping = aes(x = reorder(Age1stCode,ConvertedCompYearly,FUN=median), y =ConvertedCompYearly))+
  xlab("Age 1st Code")+
  geom_boxplot()+coord_flip()

```


Organization Size 
```{r}
ggplot(data = dataset, mapping = aes(x = reorder(OrgSize,ConvertedCompYearly,FUN=median), y =ConvertedCompYearly))+
  xlab("Organization size")+
  geom_boxplot()+coord_flip()
```


Age 
```{r}
ggplot(data = dataset, mapping = aes(x = reorder(Age,ConvertedCompYearly,FUN=median), y =ConvertedCompYearly))+
  xlab("Age")+
  geom_boxplot()+coord_flip()
```


Looks like that there are some respondents who are under 18 years old. I am skeptic that there are professional developers who are under 18 years old. In this project, I consider them errors. Moreover, There are only two respondents who fall in the category. So, it is safe to drop them. 
```{r}
#4 persons are under 18 
sum(dataset$Age=="Under 18 years old")
#drop them 
dataset <- dataset[dataset$Age!="Under 18 years old",]
```
Gender 
```{r }
ggplot(data = dataset,aes(x = Gender, y=ConvertedCompYearly))+
  geom_boxplot()+coord_flip()
```

Trans 
```{r}
ggplot(data = dataset, mapping = aes(x =Trans, y =ConvertedCompYearly))+
  geom_boxplot()+coord_flip()
```


Ethinicity 
```{r }
ggplot(data = dataset, mapping = aes(x =reorder(Ethnicity,ConvertedCompYearly,FUN=median), y =ConvertedCompYearly))+
  xlab("Ethnicity")+
  geom_boxplot()+coord_flip()
```


Accessibility
```{r}
ggplot(data = dataset, mapping = aes(x = Accessibility, y =ConvertedCompYearly))+
  geom_boxplot()+coord_flip()
```

Mentalhealth 
```{r }
ggplot(data = dataset,aes(x=MentalHealth,y=ConvertedCompYearly))+
  geom_boxplot()+coord_flip()

```


## Modeling

Let's start modeling 

Since there are too many independent variables, let's select them using the backward selection. 
I apply the feature selection on numerical columns only because R considers one of dummy variables as one variable and might drop it. 
```{r backward}
numerical_data <-select_if(dataset, is.numeric)
regfit.bwd <- regsubsets(ConvertedCompYearly~. ,nvmax=45,data=numerical_data,method="backward")
regfit_bwd_summary <- summary(regfit.bwd)
```

Get the combination of variables which produce the highest adjusted R2

```{r}
which.max(regfit_bwd_summary$adjr2)
#let's use this numerical columns for our regression model
numerical_selected <- names(coef(regfit.bwd,28))[-1]
names(coef(regfit.bwd,28))[-1]
```
Prepare a dataset for modeling. 
```{r}
categorical <-names(select_if(dataset,is.character))
variables <- c(numerical_selected,categorical)
regression_df <- subset(dataset,select=variables)
regression_df$ConvertedCompYearly <- dataset$ConvertedCompYearly
```

## First Model 

Adjusted R-squared is 0.3931 

Let’s look at YearsCode and YearsCodePro because they are this research's primary interest. YearsCodePro is statistically significant at 5% significance level. By one year increase in professional experience, the yearly compensation raises by $2042 on average, all else equal. On the other hand, YearsCode is significant at 10 % level and the coefficient estimate is much smaller than YearsCodePro. 

When looking at other columns, different variables affect the amount of compensation. States where developers live, types of developers, organization size, age, gender, and mental health affect the amount of compensation.

Check plots regarding this regression model next. 

```{r}
reg1 <- lm(ConvertedCompYearly~.,data=regression_df)
stargazer(reg1,type = "text")
```

Make several plots regarding the model. 
Although there are no outliers since no square root of standardized residuals is more than 3, it looks like that there is heteroskadasticity. Let's see what causes it.  
```{r}
plot(reg1)
```

Check what causes the residual to vary aross fitted values
Age
```{r check_residual,fig.height=6,fig.width=10}
hetero_check <- data.frame(dataset)
hetero_check$residual <- reg1$residuals
hetero_check$fitted <- reg1$fitted.values
ggplot(hetero_check,aes(x=fitted,y=residual,color=Age))+
  geom_point()
```
Education level
```{r fig.height=6,fig.width=10}
ggplot(hetero_check,aes(x=fitted,y=residual,color=EdLevel))+
  geom_point()
```

Years of experience as professional
```{r fig.height=6,fig.width=10}
ggplot(hetero_check,aes(x=fitted,y=residual,color=YearsCodePro))+
  geom_point()
```

Years of coding 
```{r fig.height=6,fig.width=10}
ggplot(hetero_check,aes(x=fitted,y=residual,color=YearsCode))+
  geom_point()
```

Employment Type
```{r fig.height=6,fig.width=10}
ggplot(hetero_check,aes(x=fitted,y=residual,color=Employment))+
  geom_point()
```

Organization size 
```{r fig.height=6,fig.width=10}
ggplot(hetero_check,aes(x=fitted,y=residual,color=OrgSize))+
  geom_point()
```
Looks like professional code experience and age cause this situation. Let's solve this issue later. I also want to check if there is multicollinearity. I use VIF score. 

It does not look like there is such an issue because no score exceeds 10. 
```{r vif}
vif(reg1)
```
 
## Model after dropping unsignificant variabels from the frist 
 
Adjusted R-squared is 0.3922 
 
Let's drop Trans ad Ethnicity because they are insignificant and their boxplots do not show significant difference. 
 
```{r}
 regression_df <-subset(regression_df,select=-c(Age1stCode,Trans,Ethnicity,Accessibility))
```
 

Run a new model with new variables. There is not a significant change. 
```{r}
model_2nd <- lm(ConvertedCompYearly~.,data=regression_df)
stargazer(model_2nd,type = "text")
```
 
Look at the plots
```{r}
plot(model_2nd)
```
 
Given the standardized residuals plot, there is no outlier. But, what about high leverage values? They tend to distort the linear model. So, if any, let's drop them. 
```{r leverage}
hv<-as.data.frame(hatvalues(model_2nd))
mn<-mean(hatvalues(model_2nd))
hv$warn <- ifelse(hv[, 'hatvalues(model_2nd)']>3*mn, 'x3',
                  ifelse(hv[, 'hatvalues(model_2nd)']>2*mn, 'x3', '-' ))
hv_high <- subset(hv, warn%in%c("x2", "x3"))
regression_df <-regression_df[!rownames(regression_df) %in% rownames(hv_high),]
```
 
## Model without high leverage observations

Run the model. There is not a significant change. 
```{r }
model_3rd <-lm(ConvertedCompYearly~.,data=regression_df)
stargazer(model_3rd,type = "text")
```

```{r}
plot(model_3rd)
```

 Check residual and fitted values plot again. 
```{r fig.height=6,fig.width=10}
hetero_check_reg3 <- data.frame(regression_df)
hetero_check_reg3$residual <- model_3rd$residuals
hetero_check_reg3$fitted <- model_3rd$fitted.values
ggplot(hetero_check_reg3,aes(x=fitted,y=residual,color=YearsCodePro))+
  geom_point()
```
 Age
```{r fig.height=6,fig.width=10}
hetero_check_reg3$fitted <- model_3rd$fitted.values
ggplot(hetero_check_reg3,aes(x=fitted,y=residual,color=Age))+
  geom_point()
```
 
## Model with log transformation on both independent and dependent variables. 
 
Let's apply log transformation on YearsCodePro, YearsCode, and the dependent variable.This is because, in the scatter plots shown in the previous section, non-linear relationships were observed. 

Apply log transformation
```{r}

no_leverage_df_log_log <- data.frame(regression_df)
no_leverage_df_log_log$log_compensation <- log(no_leverage_df_log_log$ConvertedCompYearly)
no_leverage_df_log_log$log_yearscode <- log(no_leverage_df_log_log$YearsCode)
no_leverage_df_log_log$log_yearscodepro <- log(no_leverage_df_log_log$YearsCodePro)


no_leverage_df_log_log<-subset(no_leverage_df_log_log,select=-c(ConvertedCompYearly, YearsCode, YearsCodePro))

```
 
Run the model.

Adjusted R2	is 0.432.

Since I applied the log transformation, the interpretations on the log transformed coefficient estimates are changed. They are interpreted in terms of percent change. For instance, by one percent increase in professional coding experience, the yearly compensation increases by 0.181%. 


```{r}
model_log_log <- lm(log_compensation~., data=no_leverage_df_log_log)
stargazer(model_log_log,type = "text")
```
 

Make plots regarding the model
 
The red line in the residuals and fitted values plot becomes non linear and the residuals tend to be negative from a certain fitted value, 12. I think that this is because the growth of salary amount get smaller and smaller as the salary amount increases. So, from a certain point, there is no linear relationship. This phenomenon is also seen in the scatter plot between professional experience and the amount of compensation. 

Moreover, from this plot, I can see that the variance of errors is not still constant. I might want to utilize the robust regression model to make our p-values accurate. Let's apply that.  
```{r}
plot(model_log_log)
```
 
The result of this robust regression model should be considered the best accurate model of all. This is because, previous models are likely to suffer from heteroskadasticity and it causes calculation of p-values to be inaccurate. By using the robust model, the problem should be solved. Thus, I use a result from this robust model to draw an answer for my research question. 

Log transformed years of coding experience is not statistically significant. On the other hand log transformed years of professional experience is still significant at 1 % level. This tells that professional expericen matters more than the mere coding experience. The coefficient estimate is same as the previous model because we just added white standard errors to the model. By one percent increase in professional coding experience, the yearly compensation increases by 0.181%. 

```{r}
model_log_log_robust <- coeftest(model_log_log, vcovHC)
stargazer(model_log_log_robust,type = "text")
model_log_log_robust
##browseURL('model_log_log_robust.doc')
```

## Result 

Through this project, I found that professional experience of coding positively influence the yearly amount of compensation. For each one percent increase in the professional experience of coding, there is an increase of 0.181% in the compensation amount. This is statistically significant at the 1% level. On the other hand, years of mere coding experience do not have statistically significant influence on the amount. 

However, it is observed that there is a non linear relationship between years of professional experience and the compensation amount from a certain point of experience. Moreover, it was seen that the linear model overestimates the compensation when it estimated large compensation amounts, amounts over 12 in the log scale.  

## Implications

For future researches, I would recommend to create a model that explains that non-linearity. 


## Conclusion

My research question is how does years of experience of developers affect their salaries in the U.S.? 

Through this project, I was able to find the significance of professional experience of coding on the yearly compensation amount of developers in the U.S. However, the interpretation about the effect on the compensation was only applied to developers who make less than 12 compensation in log scale. This was due to the non-linearities I discussed. Moreover, I also found out that years of coding experience does not significantly affect the compensation. 

The theory I established before the project actually matched with what I observed in this project. There was a linearity in the compensation by a certain point, but from that point the amount stays same or decreases. Although it is hard to know what causes the change, one possible explanation is that when developers reach that point, they are somewhat old and start taking less pad jobs that allow them to have more personal time. 


## References

Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in Regression Relationships. R News
  2(3), 7-10. URL https://CRAN.R-project.org/doc/Rnews/

Allaire, J. J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., & Iannone, R. (2020). rmarkdown: Dynamic Documents for R. https://github.com/rstudio/rmarkdown

Dash, M., Bakshi, S., & Chugh, A. (2017, Nov). The Relationship Between Work Experience and Employee Compensation: A Case Study of The Indian IT Industry, Journal of Applied Management and Investment, vol 6(1), 5-10. Retrieved from 
https://econpapers.repec.org/article/odsjournl/v_3a6_3ay_3a2017_3ai_3a1_3ap_3a5-10.htm

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2022). dplyr: A Grammar of Data Manipulation. R package version 1.0.8. https://CRAN.R-project.org/package=dplyr

Hadley Wickham (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr

Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer

H. Wickham. (2016, n.d.). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag 

Yihui Xie (2021, n.d.). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.37.

Max Kuhn (2022). caret: Classification and Regression Training. R package version 6.0-92. https://CRAN.R-project.org/package=caret
  
R Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. Retrieved from https://www.R-project.org/

Salary.com. (n.d.). Eight Factors That Can Affect Your Pay. Retrieved from https://www.salary.com/articles/eight-factors-that-can-affect-your-pay/

Sauro, J. (2014, Nov). User Experience Salaries & Calculator (2014). Measuring U. Retrieved from https://measuringu.com/salary-survey2014/

Stefan Milton Bache and Hadley Wickham (2020). magrittr: A Forward-Pipe Operator for R. R package version 2.0.1. https://CRAN.R-project.org/package=magrittr
  
Thomas Lumley based on Fortran code by Alan Miller (2020). leaps: Regression Subset Selection. R package version 3.1. https://CRAN.R-project.org/package=leaps

Torpey, E. (2015, May). Same occupation, different pay: How wages vary. U.S. BUREAU OF LABOR STATISTICS. Retrieved from https://www.bls.gov/careeroutlook/2015/article/wage-differences.htm

Wickham, H. (2011). The Split-Apply-Combine Strategy for Data Analysis. In Journal of Statistical Software (Vol. 40, Issue 1, pp. 1–29). http://www.jstatsoft.org/v40/i01/
  
Zeileis A, Köll S, Graham N (2020). “Various Versatile Variances: An Object-Oriented Implementation
of Clustered Covariances in R.” _Journal of Statistical Software_, *95*(1), 1-36. doi:
10.18637/jss.v095.i01 (URL: https://doi.org/10.18637/jss.v095.i01)citation()

  







